---
title: "Hello World"
author: "Ollie Khakwani"
date: "2023-04-15"
draft: true
---

# Outline

- Preamble (KEEP SHORT)
-- Taking FastAI class, paused at Lecture 4 to try out NLP and model building
-- This is post 1/n: already lots of learnings, want to get down sooner vs later

- Project description
-- Possible to predict engagement with tweet based on content?

- Process
-- Dove in without baseline model, got bit lost... then read Radek's book - recommend to others (also in FastAI class; I just forgot)
-- Realised should make baseline model first, then use basic deberta, then iterate
-- Defined some project and learning goals, ideas for baseline
-- And specific choices about project (eg just 1 user for now; choosing validation set)

- Data collection and exploration
-- Discoveries, eg initially thinking about view count

- Building baseline model
-- Tried and failed with mean/median
-- Tweet length (0) correlation
-- GPT idea: bag of words, actually great baseline model!

- Making first DL model, performance so far

- Learnings summary (with links back to areas?)
-- Spend more time exploring data first, at least basic stuff (also feels like part of process to discover things as working, but for sure wasn't thorough enough)
-- GPT was great, feels right using it for just-in-time learning pandas vs reading a pandas book first. If got truly stuck or cracks in conceptual knowledge really starting to bite, then would have gone back
-- Training failed when labels were too large and spread out; log made it work again. Think it was error just being so enormous to then apply gradient descent to
-- Out-of-memory errors, resolutions
-- Avoiding overfitting (after overfitting): comparison of model perf on test set with and without overfitting, huge gap
-- Starting with baseline

- Where next
-- Some own thoughts
-- Also Got ideas from GPT on what next, will share more then!